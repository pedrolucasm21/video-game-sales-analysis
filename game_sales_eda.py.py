# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOBC1oEsg3A7L4Nus0t2l8aMlJNxshpL
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px

# Load the dataset
# The dataset contains video game sales information up to December 22, 2016.
data = pd.read_csv('/content/Video_Games_Sales_as_at_22_Dec_2016.csv')
# Display the first few rows of the dataframe to understand its structure
data.head()

# Print the total number of games in the dataset
print(f"The total of games is {data['Name'].nunique()}.")

# Get a concise summary of the DataFrame, including data types and non-null values
data.info()

# Display unique values in 'Year_of_Release' to identify potential issues (e.g., 'N/A' or mixed types)
data['Year_of_Release'].unique()

# Data Cleaning: Handle missing values and convert 'Year_of_Release' to integer
# Drop rows where 'Year_of_Release' is NaN
data.dropna(subset=['Year_of_Release'], inplace=True)
# Convert 'Year_of_Release' to integer type
data['Year_of_Release'] = data['Year_of_Release'].astype(int)
# Verify unique values after cleaning
data['Year_of_Release'].unique()

# Analyze game releases over the years
# Count the number of games released each year and plot it as a line graph
data['Year_of_Release'].value_counts().sort_index().plot(kind='line')

# Analyze global sales by year
# Group data by 'Year_of_Release' and sum 'Global_Sales' to find total sales per year
# Display the top 10 years with the highest global sales
data.groupby('Year_of_Release')['Global_Sales'].sum().sort_values(ascending=False).head(10)

# Analyze Genres
# Get the number of unique genres in the dataset
data['Genre'].nunique()

# Determine the best-selling genre globally
# Group data by 'Genre' and sum 'Global_Sales'
best_selling_genre = data.groupby('Genre')['Global_Sales'].sum().sort_values(ascending=False)
# Display the best-selling genres
best_selling_genre

# Visualize global sales by genre using a bar chart
px.bar(best_selling_genre, x=best_selling_genre.index, y=best_selling_genre.values)

# Regional Sales Analysis
# Identify columns related to regional sales
columns_info = list(data.loc[:,'NA_Sales':'Other_Sales'].columns)
columns_info

# Calculate favorite genre by sales for each major region
fav_in_na = data.groupby('Genre')['NA_Sales'].sum().sort_values(ascending=False)
fav_in_eu = data.groupby('Genre')['EU_Sales'].sum().sort_values(ascending=False)
fav_in_jp = data.groupby('Genre')['JP_Sales'].sum().sort_values(ascending=False)
fav_in_others = data.groupby('Genre')['Other_Sales'].sum().sort_values(ascending=False)
# Print the top favorite genre for each region
print(f'The favorite genre in North America is {fav_in_na.index[0]}.')
print(f'The favorite genre in Europe is {fav_in_eu.index[0]}.')
print(f'The favorite genre in Japan is {fav_in_jp.index[0]}.')
print(f'The favorite genre in Other Regions is {fav_in_others.index[0]}.')

# Aggregate regional sales by genre
genre_by_region = data.groupby('Genre')[columns_info].sum()
genre_by_region

# Visualize regional sales distribution per genre using subplots
genre_by_region.plot(subplots=True, figsize=(20,10), kind='bar')

# Publisher Analysis: Top 100 Games by Global Sales
# Group by game Name and Publisher, sum Global Sales, and get the top 100 entries (index 0 to 100)
top100 = data.groupby(['Name','Publisher'])['Global_Sales'].sum().sort_values(ascending=False).head(101)
# Reset index to convert grouped object into a DataFrame
top100_reset = top100.reset_index()
# Count the occurrences of each publisher within the top 100 games
top100_publisher = top100_reset['Publisher'].value_counts()
top100_publisher

# Publisher Analysis: Performance outside the top 100 games
# Get games outside the top 100 global sales
out_of_top100 = data.groupby(['Name','Publisher'])['Global_Sales'].sum().sort_values(ascending=False)[101:]
out_of_top100_reset = out_of_top100.reset_index()
# Filter for publishers that appeared in the initial top 100 analysis to see their performance outside it
filter = out_of_top100_reset.query('Publisher in ["Nintendo", "Take-Two Interactive", "Activision", "Microsoft Game Studios", "Bethesda Softworks", "Electronic Arts", "LucasArts", "Sony Computer Entertainment", "Warner Bros. Interactive Entertainment", "Sega", "Ubisoft", "505 Games"]')
# Count the occurrences of these specific publishers outside the top 100
top100_publisher_extra = filter['Publisher'].value_counts()
top100_publisher_extra

# Visualize publisher performance in and out of the top 100
# Create a DataFrame for plotting comparison
publisher_df = pd.DataFrame({'Publisher': top100_publisher.index,
                              'top100_publisher': top100_publisher.values,
                              'top100_publisher_extra': top100_publisher_extra.values})
# Plot comparison using a grouped bar chart
px.bar(publisher_df, x='Publisher', y=['top100_publisher_extra', 'top100_publisher'], barmode='group', color_discrete_sequence=['cornflowerblue', 'purple'])

# Identify other publishers appearing frequently outside the top 100
# Filter for publishers NOT in the previously defined list and count their occurrences
out_of_top100_publisher = out_of_top100_reset.query('Publisher not in ["Nintendo", "Take-Two Interactive", "Activision", "Microsoft Game Studios", "Bethesda Softworks", "Electronic Arts", "LucasArts", "Sony Computer Entertainment", "Warner Bros. Interactive Entertainment", "Sega", "Ubisoft", "505 Games"]')
out_of_top100_publisher['Publisher'].value_counts()[:10]

# User and Critic Score Analysis
# Display unique values in 'User_Score' to identify non-numeric entries like 'tbd'
data['User_Score'].unique()

# Data Cleaning: Process 'User_Score'
# Replace 'tbd' (to be determined) with NaN (Not a Number)
data['User_Score'] = data['User_Score'].replace('tbd', np.nan)
# Drop rows where 'User_Score' is NaN
data.dropna(subset=['User_Score'], inplace=True)
# Convert 'User_Score' to float type
data['User_Score'] = data['User_Score'].astype(float)
# Verify unique values after cleaning
data['User_Score'].unique()

# Data Cleaning: Handle missing values and convert types for Critic_Score, Critic_Count, User_Count
# Drop rows with NaN values in these columns
data.dropna(subset=['Critic_Score'], inplace=True)
data.dropna(subset=['Critic_Count'], inplace=True)
data.dropna(subset=['User_Count'], inplace=True)
# Convert count columns to integer type
data['Critic_Count'] = data['Critic_Count'].astype(int)
data['User_Count'] = data['User_Count'].astype(int)

# Display updated DataFrame information after extensive cleaning
data.info()

# Visualize the relationship between Critic Score and User Score
# Use a scatter plot, hovering over points reveals Name and Genre
px.scatter(data, x='Critic_Score', y='User_Score', hover_data=['Name', 'Genre'])

# Analyze average Critic Score by Genre
# Group by 'Genre' and calculate the mean of 'Critic_Score', then sort
data.groupby(['Genre'])['Critic_Score'].mean().sort_values(ascending=False)

# Analyze average User Score by Genre
# Group by 'Genre' and calculate the mean of 'User_Score', then sort
data.groupby(['Genre'])['User_Score'].mean().sort_values(ascending=False)

